{
  "memories": [
    {
      "type": "completed_feature",
      "content": "Waves 1-10 COMPLETE (2026-01-14, commit f8381ce). Backend APIs, Market/Reporting, React Query, 18 UI components, E2E testing (258 Playwright), Performance, Security (RBAC, token blacklist, eval() RCE fix), LazyImage, usePrefetch, React.memo, VirtualList. Tests: 337 frontend + 1055 backend.",
      "added_at": "2026-01-14T15:00:00.000000",
      "source": "consolidated"
    },
    {
      "type": "user_preference",
      "content": "conda env: /home/mattb/miniconda3/envs/dashboard-backend. Backend port 8000, Frontend port 5173. Git: stash apply over pop, don't push unless asked. User prefers validation/testing before new features. No agent swarms for sequential dependent tasks. Be extremely clear about what actions user needs to take; state clearly if no action needed. User prefers directness over hedging. Dashboard purpose: comprehensive deal intelligence platform tracking ALL deals pursued (not just owned). Two currently owned deals: Cabana on 99th (Glendale, AZ) and Tempe Metro (Tempe, AZ). '4) Closed Deals' = currently owned, '5) Realized Deals' = sold/disposed. Do NOT expand extraction filter to older UW Model versions — different layouts would produce errors. Future task: group models by version for separate extraction. Kill port: use 'fuser -k 8000/tcp' (lsof -t may not work on this system). User is adamant: NO mock data, NO fake data, NO silent fallbacks anywhere. Remove all fabricated trend data (sine waves, etc.).",
      "added_at": "2026-02-03T06:00:00.000000",
      "source": "consolidated"
    },
    {
      "type": "tech_decision",
      "content": "pydantic-settings 2.x: CORS_ORIGINS must be JSON array. Single .env in project root, symlinked from backend/.env. Shell env var SHAREPOINT_SITE_URL overrides .env with truncated value - MUST unset shell var before running extraction. DB: PostgreSQL localhost:5432/dashboard_interface_data (NOT SQLite — dashboard.db is 0 bytes). Cell reference file: Underwriting_Dashboard_Cell_References.xlsx in project root. VITE_API_URL fix: src/lib/config.ts always appends /api/v1 to base URL; api.ts imports API_URL from config.ts as single source of truth. Email domain: bandrcapital.com (NOT brcapital.com). CI requires: ruff check + ruff format on backend, tsc -b + vite build on frontend. Stale .pyc files can prevent --reload from picking up changes — delete __pycache__ when routes aren't matching. Redis warning (token blacklist) is harmless — falls back to in-memory store for dev.",
      "added_at": "2026-02-03T06:00:00.000000",
      "source": "consolidated"
    },
    {
      "type": "extraction_status",
      "content": "EXTRACTION COMPLETE (2026-02-02): 28/28 files extracted, 0 failures, 52.8s total. 26 unique properties with extraction data + 3 monitored files with no extraction data (Casitas at San Marcos, Cortland Arrowhead Summit, Town Center Apartments). NOI in extracted_values is ANNUAL PER-UNIT (multiply by total_units for total annual NOI). Some UW models have bracket placeholders ([Year Built], [City], etc.) instead of real data — _is_placeholder() detects these. Filters: FILE_PATTERN=.*UW.*Model.*vCurrent.*, CUTOFF_DATE=2024-07-15. 28 files pass (23 dead, 5 initial_review), 253 skipped. Key field corrections: Cash-on-Cash must use LP_RETURNS_PREREFI_CASH_ON_CASH (rate, e.g. 0.0747), NOT EQUITY_CASH_ON_CASH_EXCLUDING_PROJECTLEVEL_FEES_YEAR_1 (dollar amount, e.g. -26252).",
      "added_at": "2026-02-03T06:00:00.000000",
      "source": "consolidated"
    },
    {
      "type": "current_status",
      "content": "DATA ACCURACY FIXES COMPLETE (2026-02-03). 30 properties, 30 deals, 28 documents, 2 transactions. All 30 have exact GPS coordinates. Fixes applied: (1) Cash-on-Cash uses LP_RETURNS_PREREFI_CASH_ON_CASH (rate), (2) Occupancy normalized to 0-1 scale in backend (DB stores whole percent 93.0, API returns 0.93), (3) Deals page fixed — page_size changed 200→100 (backend le=100 validation was returning 422) AND useState stale data bug fixed with useMemo, (4) PhoenixSubmarket updated to 15 CoStar Submarket Clusters using authoritative mapping from docs/Dashboard Project - Basic Deal Info.xlsx, (5) Analytics: removed fabricated sine-wave trends, replaced with real portfolio summary, (6) Underwrite button: bg-accent-600→bg-blue-600 for visibility, (7) Market page: added 'Static Reference Data' banner, updated to 15 CoStar submarkets. All 328 frontend tests pass, tsc/vite build clean. Stage mapping: dead→closed_lost, initial_review→underwriting, closed→closed_won. Next: user reviewing pages one at a time for remaining issues. Market page needs third-party data source integration (CoStar, Census, BLS) as follow-up. Transactions page merge into Investments is also follow-up.",
      "added_at": "2026-02-03T06:00:00.000000",
      "source": "consolidated"
    }
  ],
  "manual_memories": [],
  "realtime_memories": [
    {
      "type": "claude_response",
      "content": "All 8 data accuracy fixes from the plan are complete and verified:\n\n1. **Cash-on-Cash** — Fixed to use `LP_RETURNS_PREREFI_CASH_ON_CASH` (rate) instead of dollar amount\n2. **Occupancy** — Normalized t",
      "added_at": "2026-02-03T02:08:38.207103",
      "source": "realtime_capture"
    },
    {
      "type": "message",
      "content": "What impact does the below error language have on the functionality of my dashboard when I run it: \"2026-02-02 23:41:25.551 | WARNING | app.core.token_blacklist:_init_redis:49 - Token blacklist: Redis",
      "added_at": "2026-02-03T02:08:38.207142",
      "source": "realtime_capture"
    },
    {
      "type": "claude_response",
      "content": "\n\n**Redis warning** — No functional impact. The token blacklist (used for JWT logout/invalidation) tries to connect to Redis, fails, and falls back to an in-memory store. This works fine for developme",
      "added_at": "2026-02-03T02:10:35.027852",
      "source": "realtime_capture"
    },
    {
      "type": "message",
      "content": "I have another pretty lengthy list, but I am going to provide my comments onepage at a time and hopefully that keeps you focused and fixing those issues so we can finalize and move tothe next page\" 1.",
      "added_at": "2026-02-03T02:10:35.027922",
      "source": "realtime_capture"
    },
    {
      "type": "claude_response",
      "content": "Here's what I fixed on the Dashboard page:\n\n**Changes made:**\n\n1. **Removed fake growth indicators** — The hero stat cards had hardcoded `+12.4%` (Portfolio Value) and `+8.2%` (Monthly NOI) badges tha",
      "added_at": "2026-02-03T02:21:04.304604",
      "source": "realtime_capture"
    },
    {
      "type": "message",
      "content": "Each time we finish working through the tasks/steps for a given page, let's then leverage an agent team or swarm to perform reviewing, testing, asking questions, etc. on the page we just finished. Wha",
      "added_at": "2026-02-03T02:21:04.304710",
      "source": "realtime_capture"
    },
    {
      "type": "claude_response",
      "content": "Here's what I'd recommend — focused on what actually catches real issues vs. ceremony:\n\n**Effective parallel checks per page (3-4 agents):**\n\n1. **Visual validation agent** — Uses Playwright MCP to na",
      "added_at": "2026-02-03T02:22:52.293400",
      "source": "realtime_capture"
    },
    {
      "type": "message",
      "content": "I'm good with your proposed workflow. As such, let's move forward with it.",
      "added_at": "2026-02-03T02:22:52.293437",
      "source": "realtime_capture"
    }
  ],
  "created_at": "2026-01-07T02:10:53.387854",
  "updated_at": "2026-02-03T02:22:52.293439"
}