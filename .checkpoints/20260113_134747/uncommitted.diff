diff --git a/.claude/memories/project_memory.json b/.claude/memories/project_memory.json
index 8e1c9b2..044d985 100644
--- a/.claude/memories/project_memory.json
+++ b/.claude/memories/project_memory.json
@@ -2,32 +2,32 @@
   "memories": [
     {
       "type": "completed_feature",
-      "content": "Extraction API sync/async fix completed (2026-01-06). Added get_sync_db() to db/session.py, updated 6 extraction endpoints. O(nÂ²) â†’ O(1) cell lookup optimization implemented with sheet caching. Performance: 1.04s for 14MB file, 99.5% cache hit rate.",
-      "added_at": "2026-01-07T06:45:00.000000",
+      "content": "Extraction API with O(1) cell lookup optimization (2026-01-06). Sync/async session handling via get_sync_db(). Performance: 1.04s for 14MB file, 99.5% cache hit rate. Extraction endpoint refactored from 1153 lines into 6 modular files in backend/app/api/v1/endpoints/extraction/.",
+      "added_at": "2026-01-07T10:12:00.000000",
       "source": "consolidated"
     },
     {
-      "type": "tech_decision",
-      "content": "Security: config.py uses env vars only (no hardcoded secrets). Token blacklist implemented in app/core/token_blacklist.py with Redis primary + in-memory fallback. JWTs include jti for blacklist support. CORS configured for localhost dev + brcapital.com production domains.",
-      "added_at": "2026-01-07T06:45:00.000000",
+      "type": "completed_feature",
+      "content": "Security Hardening complete (2026-01-07, commit 7c8a87e). RBAC with 4 roles (ADMIN/MANAGER/ANALYST/VIEWER) in backend/app/core/permissions.py. Token blacklist with Redis + memory fallback. Config.py uses env vars only. Analytics endpoints use real DB queries with mock fallbacks.",
+      "added_at": "2026-01-07T10:12:00.000000",
       "source": "consolidated"
     },
     {
       "type": "project_status",
-      "content": "Tests: 1004 passed, 16 skipped. Deployment readiness: ~70%. Main blockers: complete deploy.yml placeholders, create docker-compose.yml. Key files: backend/app/extraction/extractor.py (cell caching), backend/app/api/v1/endpoints/extraction.py (18 endpoints), backend/app/core/config.py (env-based config).",
-      "added_at": "2026-01-07T06:45:00.000000",
+      "content": "Tests: 159 API tests, 37.15% coverage. Docker/deployment DEFERRED (2026-01-13) - all Docker files removed, deploy.yml.disabled. Checkpoint saved: 20260113_130126 (docker-removed). Created scripts/start.sh for combined frontend+backend startup. Repo: mborgeson/dashboard_interface_project.",
+      "added_at": "2026-01-13T13:15:00.000000",
       "source": "consolidated"
     },
     {
-      "type": "outstanding_tasks",
-      "content": "Priority: 1) Complete deploy.yml placeholder scripts, 2) Create docker-compose.yml for local dev, 3) Add tests for 7 monitor endpoints, 4) Split extraction.py (1154 lines) into smaller files, 5) Create test_crud/test_extraction.py and test_services/test_file_monitor.py.",
-      "added_at": "2026-01-07T06:45:00.000000",
+      "type": "user_preference",
+      "content": "Development environment: User uses conda environment 'dashboard-backend' (not venv). Run './scripts/start.sh' to start both servers. Backend: uvicorn on port 8000. Frontend: Vite on port 5173. Ctrl+C stops both.",
+      "added_at": "2026-01-13T13:15:00.000000",
       "source": "consolidated"
     },
     {
       "type": "convention",
-      "content": "SharePoint extraction uses memory-keeper for checkpoints (namespace: dashboard-project). Extraction scheduler at EXTRACTION_SCHEDULE_CRON (default 2 AM). File filter: .*UW.*Model.*vCurrent.* pattern, excludes ~$,.tmp,backup,vOld. Reference file: Underwriting_Dashboard_Cell_References.xlsx with 1169 cell mappings.",
-      "added_at": "2026-01-07T06:45:00.000000",
+      "content": "SharePoint extraction: memory-keeper checkpoints (namespace: dashboard-project). Scheduler at EXTRACTION_SCHEDULE_CRON (default 2 AM). File filter: .*UW.*Model.*vCurrent.*. Reference: Underwriting_Dashboard_Cell_References.xlsx (1169 cell mappings). Env vars: see backend/app/core/config.py.",
+      "added_at": "2026-01-07T14:55:00.000000",
       "source": "consolidated"
     }
   ],
@@ -35,29 +35,89 @@
   "realtime_memories": [
     {
       "type": "claude_response",
-      "content": "Memory consolidated: 11 â†’ 5 memories. Ready to continue or commit changes.",
-      "added_at": "2026-01-07T07:10:14.670394",
+      "content": "No, the servers are not currently running. To start them:\n\n```bash\n./scripts/start.sh\n```",
+      "added_at": "2026-01-13T13:18:58.333912",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "message",
+      "content": "B&R Capital Dashboard - Starting... ======================================== /home/mattb/projects/dashboard_interface_project/scripts/start.sh: line 34: conda: command not found Activating conda envir",
+      "added_at": "2026-01-13T13:18:58.333954",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "claude_response",
+      "content": "Fixed. Now try again:\n\n```bash\n./scripts/start.sh\n```",
+      "added_at": "2026-01-13T13:23:27.201311",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "message",
+      "content": "Where do I run this command from, \"./scripts/start.sh\"?",
+      "added_at": "2026-01-13T13:23:27.201330",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "claude_response",
+      "content": "From the project root directory:\n\n```bash\ncd ~/projects/dashboard_interface_project\n./scripts/start.sh\n```\n\nOr from anywhere using the full path:\n\n```bash\n~/projects/dashboard_interface_project/script",
+      "added_at": "2026-01-13T13:27:07.220838",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "message",
+      "content": "î‚¶ó°•ˆ mattbî‚° ~ î‚° ï˜ main î‚° îœ˜ v22.21.0 î˜† v3.12.12 î‚° ï„Œ dashboard-backend î‚° ïº 13:25 î‚´ â¯ ~/projects/dashboard_interface_project/scripts/start.sh ======================================== B&R Capital Dashboard ",
+      "added_at": "2026-01-13T13:27:07.220931",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "claude_response",
+      "content": "Fixed. The change will hot-reload automatically since Vite is running.\n\n**Refresh your browser** (or it may have already updated). The infinite loop should be resolved.\n\nThe bug was: `results` array g",
+      "added_at": "2026-01-13T13:31:30.548701",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "message",
+      "content": "Perfect. It ran. Please create a checkpoint for this.",
+      "added_at": "2026-01-13T13:31:30.548721",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "claude_response",
+      "content": "Done.\n\n```\nCheckpoint: 20260113_133244\nName: globalsearch-fix\nMessage: Fixed infinite loop in GlobalSearch component\n```",
+      "added_at": "2026-01-13T13:33:24.633282",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "message",
+      "content": "Please review the curretn stage of database integration into the dashboard and what next steps would be to ensuring seamless integration, removal of mock data, and any other recommended steps to take.",
+      "added_at": "2026-01-13T13:33:24.633318",
+      "source": "realtime_capture"
+    },
+    {
+      "type": "claude_response",
+      "content": "## Database Integration Review\n\n### Current Status: ~40% Integrated\n\n| Feature | Backend | Frontend Hook | Page Using API | Status |\n|---------|---------|---------------|----------------|--------|\n| *",
+      "added_at": "2026-01-13T13:44:25.142678",
       "source": "realtime_capture"
     },
     {
       "type": "message",
-      "content": "Let's continue with the remaining items.",
-      "added_at": "2026-01-07T07:10:14.670417",
+      "content": "Please move forward with â€œ2. Create a full implementation plan for all missing integrationsâ€, however, prior to moving forward with approach, please let me know if the items listed under â€œOther Issues",
+      "added_at": "2026-01-13T13:44:25.142749",
       "source": "realtime_capture"
     },
     {
       "type": "claude_response",
-      "content": "## Summary - All Tasks Complete\n\n| Task | Status |\n|------|--------|\n| Complete deploy.yml placeholder scripts | âœ… Done |\n| Create docker-compose.yml for local dev | âœ… Already existed |\n| Add tests fo",
-      "added_at": "2026-01-07T07:28:30.093169",
+      "content": "Good question. The \"Other Issues Found\" items are **foundational issues** that should be addressed **FIRST**. Here's why:\n\n| Issue | Impact on New Integrations |\n|-------|---------------------------|\n",
+      "added_at": "2026-01-13T13:46:30.172953",
       "source": "realtime_capture"
     },
     {
       "type": "message",
-      "content": "Yes, but include it as referenced in the below request: I would like to create an automatic memory workflow/hook whereby the completion of each project Phase triggers a claude-mem save (i.e., please t",
-      "added_at": "2026-01-07T07:28:30.093213",
+      "content": "Please, but create a checkpoint first then proceed with this plan.",
+      "added_at": "2026-01-13T13:46:30.173008",
       "source": "realtime_capture"
     }
   ],
   "created_at": "2026-01-07T02:10:53.387854",
-  "updated_at": "2026-01-07T07:28:30.093215"
+  "updated_at": "2026-01-13T13:46:30.173012"
 }
\ No newline at end of file
diff --git a/.github/workflows/deploy.yml b/.github/workflows/deploy.yml
deleted file mode 100644
index c95efb4..0000000
--- a/.github/workflows/deploy.yml
+++ /dev/null
@@ -1,313 +0,0 @@
-name: Deploy
-
-on:
-  # Deploy on push to main after CI passes
-  workflow_run:
-    workflows: ["Frontend CI", "Backend CI"]
-    branches: [main]
-    types: [completed]
-
-  # Manual deployment trigger
-  workflow_dispatch:
-    inputs:
-      environment:
-        description: 'Deployment environment'
-        required: true
-        default: 'staging'
-        type: choice
-        options:
-          - staging
-          - production
-
-env:
-  REGISTRY: ghcr.io
-  IMAGE_NAME: ${{ github.repository }}
-
-jobs:
-  # Pre-deployment checks
-  pre-deploy:
-    name: Pre-deployment Checks
-    runs-on: ubuntu-latest
-    # Only run if CI workflow succeeded OR if manually triggered
-    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
-
-    outputs:
-      environment: ${{ steps.set-env.outputs.environment }}
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Set deployment environment
-        id: set-env
-        run: |
-          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
-            echo "environment=${{ inputs.environment }}" >> $GITHUB_OUTPUT
-          else
-            # Auto-deploy to staging on main branch
-            echo "environment=staging" >> $GITHUB_OUTPUT
-          fi
-
-      - name: Verify required secrets
-        run: |
-          # Check that required secrets are configured
-          # This doesn't expose secrets, just checks they exist
-          echo "Checking deployment configuration..."
-          if [ -z "${{ secrets.DEPLOY_SSH_KEY }}" ]; then
-            echo "::warning::DEPLOY_SSH_KEY not configured"
-          fi
-          if [ -z "${{ secrets.DEPLOY_HOST }}" ]; then
-            echo "::warning::DEPLOY_HOST not configured"
-          fi
-
-  # Build Docker images
-  build-images:
-    name: Build Docker Images
-    runs-on: ubuntu-latest
-    needs: pre-deploy
-
-    permissions:
-      contents: read
-      packages: write
-
-    outputs:
-      frontend-image: ${{ steps.meta-frontend.outputs.tags }}
-      backend-image: ${{ steps.meta-backend.outputs.tags }}
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Set up Docker Buildx
-        uses: docker/setup-buildx-action@v3
-
-      - name: Login to Container Registry
-        uses: docker/login-action@v3
-        with:
-          registry: ${{ env.REGISTRY }}
-          username: ${{ github.actor }}
-          password: ${{ secrets.GITHUB_TOKEN }}
-
-      - name: Extract metadata for frontend
-        id: meta-frontend
-        uses: docker/metadata-action@v5
-        with:
-          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend
-          tags: |
-            type=sha,prefix=
-            type=ref,event=branch
-            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }}
-
-      - name: Extract metadata for backend
-        id: meta-backend
-        uses: docker/metadata-action@v5
-        with:
-          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend
-          tags: |
-            type=sha,prefix=
-            type=ref,event=branch
-            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }}
-
-      - name: Build and push frontend image
-        uses: docker/build-push-action@v5
-        with:
-          context: .
-          file: ./Dockerfile.frontend
-          push: true
-          tags: ${{ steps.meta-frontend.outputs.tags }}
-          labels: ${{ steps.meta-frontend.outputs.labels }}
-          cache-from: type=gha
-          cache-to: type=gha,mode=max
-
-      - name: Build and push backend image
-        uses: docker/build-push-action@v5
-        with:
-          context: ./backend
-          file: ./backend/Dockerfile
-          push: true
-          tags: ${{ steps.meta-backend.outputs.tags }}
-          labels: ${{ steps.meta-backend.outputs.labels }}
-          cache-from: type=gha
-          cache-to: type=gha,mode=max
-
-  # Deploy to staging environment
-  deploy-staging:
-    name: Deploy to Staging
-    runs-on: ubuntu-latest
-    needs: [pre-deploy, build-images]
-    if: needs.pre-deploy.outputs.environment == 'staging'
-    environment:
-      name: staging
-      url: ${{ vars.STAGING_URL }}
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Setup SSH key
-        run: |
-          mkdir -p ~/.ssh
-          echo "${{ secrets.DEPLOY_SSH_KEY }}" > ~/.ssh/deploy_key
-          chmod 600 ~/.ssh/deploy_key
-          ssh-keyscan -H ${{ secrets.STAGING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
-
-      - name: Deploy to staging server
-        env:
-          DEPLOY_HOST: ${{ secrets.STAGING_HOST }}
-          DEPLOY_USER: ${{ secrets.DEPLOY_USER || 'deploy' }}
-          FRONTEND_IMAGE: ${{ needs.build-images.outputs.frontend-image }}
-          BACKEND_IMAGE: ${{ needs.build-images.outputs.backend-image }}
-        run: |
-          echo "ðŸš€ Deploying to staging environment..."
-
-          # Skip if no host configured (allows dry-run)
-          if [ -z "$DEPLOY_HOST" ]; then
-            echo "::warning::STAGING_HOST not configured - running in dry-run mode"
-            echo "Would deploy: Frontend=$FRONTEND_IMAGE, Backend=$BACKEND_IMAGE"
-            exit 0
-          fi
-
-          # Deploy via SSH
-          ssh -i ~/.ssh/deploy_key -o StrictHostKeyChecking=no ${DEPLOY_USER}@${DEPLOY_HOST} << 'ENDSSH'
-            cd /opt/dashboard || exit 1
-
-            # Pull latest images
-            docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:latest
-            docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:latest
-
-            # Update and restart services
-            docker-compose -f docker-compose.prod.yml pull
-            docker-compose -f docker-compose.prod.yml up -d --force-recreate backend frontend
-
-            # Clean up old images
-            docker image prune -f
-
-            echo "Staging deployment complete"
-          ENDSSH
-
-          echo "âœ… Staging deployment complete"
-
-      - name: Notify deployment
-        if: success()
-        run: |
-          echo "::notice::Successfully deployed to staging"
-
-  # Deploy to production environment
-  deploy-production:
-    name: Deploy to Production
-    runs-on: ubuntu-latest
-    needs: [pre-deploy, build-images]
-    if: needs.pre-deploy.outputs.environment == 'production'
-    environment:
-      name: production
-      url: ${{ vars.PRODUCTION_URL }}
-
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v4
-
-      - name: Setup SSH key
-        run: |
-          mkdir -p ~/.ssh
-          echo "${{ secrets.DEPLOY_SSH_KEY }}" > ~/.ssh/deploy_key
-          chmod 600 ~/.ssh/deploy_key
-          ssh-keyscan -H ${{ secrets.PRODUCTION_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
-
-      - name: Deploy to production server
-        env:
-          DEPLOY_HOST: ${{ secrets.PRODUCTION_HOST }}
-          DEPLOY_USER: ${{ secrets.DEPLOY_USER || 'deploy' }}
-          FRONTEND_IMAGE: ${{ needs.build-images.outputs.frontend-image }}
-          BACKEND_IMAGE: ${{ needs.build-images.outputs.backend-image }}
-        run: |
-          echo "ðŸš€ Deploying to production environment..."
-
-          # Skip if no host configured (allows dry-run)
-          if [ -z "$DEPLOY_HOST" ]; then
-            echo "::warning::PRODUCTION_HOST not configured - running in dry-run mode"
-            echo "Would deploy: Frontend=$FRONTEND_IMAGE, Backend=$BACKEND_IMAGE"
-            exit 0
-          fi
-
-          # Deploy via SSH with rolling update
-          ssh -i ~/.ssh/deploy_key -o StrictHostKeyChecking=no ${DEPLOY_USER}@${DEPLOY_HOST} << 'ENDSSH'
-            cd /opt/dashboard || exit 1
-
-            # Backup current state
-            docker-compose -f docker-compose.prod.yml config > /tmp/docker-compose-backup.yml
-
-            # Pull latest images
-            docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:latest
-            docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:latest
-
-            # Run database migrations
-            docker-compose -f docker-compose.prod.yml exec -T backend alembic upgrade head || echo "Migration skipped"
-
-            # Rolling update - backend first, then frontend
-            docker-compose -f docker-compose.prod.yml up -d --no-deps --force-recreate backend
-            sleep 10  # Wait for backend health check
-            docker-compose -f docker-compose.prod.yml up -d --no-deps --force-recreate frontend
-
-            # Verify health
-            sleep 5
-            curl -sf http://localhost:8000/health || echo "Backend health check pending"
-
-            # Clean up old images
-            docker image prune -f
-
-            echo "Production deployment complete"
-          ENDSSH
-
-          echo "âœ… Production deployment complete"
-
-      - name: Create deployment tag
-        if: success()
-        run: |
-          git tag -a "deploy-$(date +%Y%m%d-%H%M%S)" -m "Production deployment"
-          # git push origin --tags  # Uncomment to push tags
-
-  # Post-deployment health checks
-  health-check:
-    name: Health Check
-    runs-on: ubuntu-latest
-    needs: [deploy-staging, deploy-production]
-    if: always() && (needs.deploy-staging.result == 'success' || needs.deploy-production.result == 'success')
-
-    steps:
-      - name: Check frontend health
-        continue-on-error: true
-        run: |
-          echo "Checking frontend health..."
-          if [ -n "${{ vars.STAGING_URL }}" ]; then
-            curl -sf "${{ vars.STAGING_URL }}" -o /dev/null && echo "âœ… Frontend healthy" || echo "âš ï¸ Frontend health check failed"
-          elif [ -n "${{ vars.PRODUCTION_URL }}" ]; then
-            curl -sf "${{ vars.PRODUCTION_URL }}" -o /dev/null && echo "âœ… Frontend healthy" || echo "âš ï¸ Frontend health check failed"
-          else
-            echo "::notice::No URL configured for health check"
-          fi
-
-      - name: Check backend health
-        continue-on-error: true
-        run: |
-          echo "Checking backend health..."
-          if [ -n "${{ vars.STAGING_URL }}" ]; then
-            curl -sf "${{ vars.STAGING_URL }}/api/v1/health" && echo "âœ… Backend healthy" || echo "âš ï¸ Backend health check failed"
-          elif [ -n "${{ vars.PRODUCTION_URL }}" ]; then
-            curl -sf "${{ vars.PRODUCTION_URL }}/api/v1/health" && echo "âœ… Backend healthy" || echo "âš ï¸ Backend health check failed"
-          else
-            echo "::notice::No URL configured for health check"
-          fi
-
-      - name: Check monitoring endpoints
-        continue-on-error: true
-        run: |
-          echo "Checking monitoring endpoints..."
-          BASE_URL="${{ vars.STAGING_URL || vars.PRODUCTION_URL }}"
-          if [ -n "$BASE_URL" ]; then
-            curl -sf "${BASE_URL}/api/v1/monitoring/health/live" && echo "âœ… Liveness probe OK"
-            curl -sf "${BASE_URL}/api/v1/monitoring/health/ready" && echo "âœ… Readiness probe OK"
-          fi
-
-      - name: Report status
-        run: |
-          echo "::notice::Deployment health check complete"
diff --git a/Dockerfile.frontend b/Dockerfile.frontend
deleted file mode 100644
index 7480ada..0000000
--- a/Dockerfile.frontend
+++ /dev/null
@@ -1,34 +0,0 @@
-# Frontend Dockerfile - Multi-stage build for production
-FROM node:20-alpine AS builder
-
-WORKDIR /app
-
-# Copy package files
-COPY package*.json ./
-
-# Install dependencies
-RUN npm ci
-
-# Copy source files
-COPY . .
-
-# Build the application
-RUN npm run build
-
-# Production stage with nginx
-FROM nginx:alpine AS production
-
-# Copy custom nginx config
-COPY nginx.conf /etc/nginx/conf.d/default.conf
-
-# Copy built assets from builder stage
-COPY --from=builder /app/dist /usr/share/nginx/html
-
-# Expose port
-EXPOSE 80
-
-# Health check
-HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
-  CMD wget --no-verbose --tries=1 --spider http://localhost:80/ || exit 1
-
-CMD ["nginx", "-g", "daemon off;"]
diff --git a/backend/.claude-flow/metrics/performance.json b/backend/.claude-flow/metrics/performance.json
index 21bdf96..434d8df 100644
--- a/backend/.claude-flow/metrics/performance.json
+++ b/backend/.claude-flow/metrics/performance.json
@@ -1,7 +1,7 @@
 {
-  "startTime": 1767795403859,
-  "sessionId": "session-1767795403859",
-  "lastActivity": 1767795403859,
+  "startTime": 1767804400645,
+  "sessionId": "session-1767804400645",
+  "lastActivity": 1767804400645,
   "sessionDuration": 0,
   "totalTasks": 1,
   "successfulTasks": 1,
diff --git a/backend/.claude-flow/metrics/task-metrics.json b/backend/.claude-flow/metrics/task-metrics.json
index f46da4d..896fa01 100644
--- a/backend/.claude-flow/metrics/task-metrics.json
+++ b/backend/.claude-flow/metrics/task-metrics.json
@@ -1,10 +1,10 @@
 [
   {
-    "id": "cmd-hooks-1767795403968",
+    "id": "cmd-hooks-1767804400798",
     "type": "hooks",
     "success": true,
-    "duration": 11.505511000000013,
-    "timestamp": 1767795403980,
+    "duration": 14.041680000000042,
+    "timestamp": 1767804400812,
     "metadata": {}
   }
 ]
\ No newline at end of file
diff --git a/backend/.dockerignore b/backend/.dockerignore
deleted file mode 100644
index ffeef65..0000000
--- a/backend/.dockerignore
+++ /dev/null
@@ -1,154 +0,0 @@
-# =============================================================================
-# Docker Ignore File - B&R Capital Dashboard Backend
-# =============================================================================
-# Excludes files from Docker build context for faster builds and smaller images
-# =============================================================================
-
-# Version Control
-.git
-.gitignore
-.gitattributes
-
-# Python
-__pycache__
-*.py[cod]
-*$py.class
-*.so
-.Python
-build/
-develop-eggs/
-dist/
-downloads/
-eggs/
-.eggs/
-lib/
-lib64/
-parts/
-sdist/
-var/
-wheels/
-*.egg-info/
-.installed.cfg
-*.egg
-
-# Virtual Environments
-venv/
-.venv/
-ENV/
-env/
-.env.local
-.env.development
-.env.test
-
-# IDE and Editor
-.idea/
-.vscode/
-*.swp
-*.swo
-*~
-.project
-.pydevproject
-.settings/
-
-# Testing
-.coverage
-.pytest_cache/
-htmlcov/
-.tox/
-.nox/
-coverage.xml
-*.cover
-*.py,cover
-.hypothesis/
-tests/
-test_*.py
-*_test.py
-
-# Documentation
-docs/
-*.md
-!README.md
-LICENSE
-CHANGELOG.md
-CONTRIBUTING.md
-
-# CI/CD
-.github/
-.gitlab-ci.yml
-.travis.yml
-Jenkinsfile
-azure-pipelines.yml
-
-# Docker (avoid recursive includes)
-Dockerfile*
-docker-compose*.yml
-.docker/
-.dockerignore
-
-# Local Development
-*.log
-*.logs
-logs/
-*.local
-*.bak
-*.backup
-tmp/
-temp/
-
-# Database
-*.db
-*.sqlite
-*.sqlite3
-data/
-migrations/versions/*.pyc
-
-# Static Assets (handled separately)
-static/
-media/
-uploads/
-
-# Secrets (never include in images)
-*.pem
-*.key
-*.crt
-*.env
-.env*
-!.env.example
-secrets/
-credentials/
-
-# macOS
-.DS_Store
-.AppleDouble
-.LSOverride
-._*
-
-# Windows
-Thumbs.db
-ehthumbs.db
-Desktop.ini
-
-# Jupyter Notebooks
-*.ipynb
-.ipynb_checkpoints/
-
-# mypy
-.mypy_cache/
-.dmypy.json
-dmypy.json
-
-# Profiling
-*.prof
-*.lprof
-
-# Package managers
-node_modules/
-package-lock.json
-yarn.lock
-
-# Misc
-*.bak
-*.tmp
-*.temp
-*.old
-*.orig
diff --git a/backend/Dockerfile b/backend/Dockerfile
deleted file mode 100644
index 95691d2..0000000
--- a/backend/Dockerfile
+++ /dev/null
@@ -1,125 +0,0 @@
-# =============================================================================
-# B&R Capital Dashboard Backend - Production Dockerfile
-# =============================================================================
-# Multi-stage build for optimal image size and security
-# =============================================================================
-
-# -----------------------------------------------------------------------------
-# Stage 1: Builder - Install dependencies and build wheels
-# -----------------------------------------------------------------------------
-FROM python:3.11-slim as builder
-
-# Set build-time environment variables
-ENV PYTHONDONTWRITEBYTECODE=1 \
-    PYTHONUNBUFFERED=1 \
-    PIP_NO_CACHE_DIR=1 \
-    PIP_DISABLE_PIP_VERSION_CHECK=1
-
-# Install build dependencies
-RUN apt-get update && apt-get install -y --no-install-recommends \
-    build-essential \
-    libpq-dev \
-    && rm -rf /var/lib/apt/lists/*
-
-# Create virtual environment
-RUN python -m venv /opt/venv
-ENV PATH="/opt/venv/bin:$PATH"
-
-# Install Python dependencies
-COPY requirements.txt .
-RUN pip install --upgrade pip && \
-    pip install -r requirements.txt
-
-# -----------------------------------------------------------------------------
-# Stage 2: Production - Minimal runtime image
-# -----------------------------------------------------------------------------
-FROM python:3.11-slim as production
-
-# Labels for container metadata
-LABEL maintainer="B&R Capital Development Team" \
-      version="1.0.0" \
-      description="B&R Capital Dashboard Backend API"
-
-# Set runtime environment variables
-ENV PYTHONDONTWRITEBYTECODE=1 \
-    PYTHONUNBUFFERED=1 \
-    PYTHONFAULTHANDLER=1 \
-    # Application settings
-    APP_HOME=/app \
-    APP_USER=appuser \
-    APP_GROUP=appgroup \
-    # Default configuration (override in production)
-    HOST=0.0.0.0 \
-    PORT=8000 \
-    WORKERS=4 \
-    LOG_LEVEL=info
-
-# Install runtime dependencies only
-RUN apt-get update && apt-get install -y --no-install-recommends \
-    libpq5 \
-    curl \
-    && rm -rf /var/lib/apt/lists/* \
-    && apt-get clean
-
-# Create non-root user for security
-RUN groupadd --gid 1000 ${APP_GROUP} && \
-    useradd --uid 1000 --gid ${APP_GROUP} --shell /bin/bash --create-home ${APP_USER}
-
-# Copy virtual environment from builder
-COPY --from=builder /opt/venv /opt/venv
-ENV PATH="/opt/venv/bin:$PATH"
-
-# Set working directory
-WORKDIR ${APP_HOME}
-
-# Copy application code
-COPY --chown=${APP_USER}:${APP_GROUP} . .
-
-# Create necessary directories
-RUN mkdir -p logs data && \
-    chown -R ${APP_USER}:${APP_GROUP} ${APP_HOME}
-
-# Switch to non-root user
-USER ${APP_USER}
-
-# Health check endpoint
-HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
-    CMD curl -f http://localhost:${PORT}/health || exit 1
-
-# Expose application port
-EXPOSE ${PORT}
-
-# Default command - Uvicorn with production settings
-CMD ["sh", "-c", "uvicorn app.main:app --host ${HOST} --port ${PORT} --workers ${WORKERS} --log-level ${LOG_LEVEL}"]
-
-# -----------------------------------------------------------------------------
-# Stage 3: Development - Extended image with dev tools
-# -----------------------------------------------------------------------------
-FROM production as development
-
-# Switch to root for installing dev dependencies
-USER root
-
-# Install development dependencies
-RUN apt-get update && apt-get install -y --no-install-recommends \
-    git \
-    vim \
-    && rm -rf /var/lib/apt/lists/*
-
-# Install dev Python packages
-RUN pip install \
-    pytest \
-    pytest-cov \
-    pytest-asyncio \
-    httpx \
-    black \
-    flake8 \
-    isort \
-    mypy \
-    ipython
-
-# Switch back to app user
-USER ${APP_USER}
-
-# Override command for development (with reload)
-CMD ["sh", "-c", "uvicorn app.main:app --host ${HOST} --port ${PORT} --reload --log-level debug"]
diff --git a/backend/app/api/v1/endpoints/__pycache__/analytics.cpython-312.pyc b/backend/app/api/v1/endpoints/__pycache__/analytics.cpython-312.pyc
index ccab223..07c7e4f 100644
Binary files a/backend/app/api/v1/endpoints/__pycache__/analytics.cpython-312.pyc and b/backend/app/api/v1/endpoints/__pycache__/analytics.cpython-312.pyc differ
diff --git a/backend/app/api/v1/endpoints/__pycache__/properties.cpython-312.pyc b/backend/app/api/v1/endpoints/__pycache__/properties.cpython-312.pyc
index 738d159..b5b7654 100644
Binary files a/backend/app/api/v1/endpoints/__pycache__/properties.cpython-312.pyc and b/backend/app/api/v1/endpoints/__pycache__/properties.cpython-312.pyc differ
diff --git a/backend/app/api/v1/endpoints/__pycache__/users.cpython-312.pyc b/backend/app/api/v1/endpoints/__pycache__/users.cpython-312.pyc
index e726e59..582f01b 100644
Binary files a/backend/app/api/v1/endpoints/__pycache__/users.cpython-312.pyc and b/backend/app/api/v1/endpoints/__pycache__/users.cpython-312.pyc differ
diff --git a/backend/app/models/__pycache__/user.cpython-312.pyc b/backend/app/models/__pycache__/user.cpython-312.pyc
index 9b65118..15e7560 100644
Binary files a/backend/app/models/__pycache__/user.cpython-312.pyc and b/backend/app/models/__pycache__/user.cpython-312.pyc differ
diff --git a/backend/app/schemas/__pycache__/user.cpython-312.pyc b/backend/app/schemas/__pycache__/user.cpython-312.pyc
index 5c5866a..23a73da 100644
Binary files a/backend/app/schemas/__pycache__/user.cpython-312.pyc and b/backend/app/schemas/__pycache__/user.cpython-312.pyc differ
diff --git a/backend/docker-compose.prod.yml b/backend/docker-compose.prod.yml
deleted file mode 100644
index 9552733..0000000
--- a/backend/docker-compose.prod.yml
+++ /dev/null
@@ -1,195 +0,0 @@
-# =============================================================================
-# B&R Capital Dashboard Backend - Production Docker Compose Overrides
-# =============================================================================
-# Usage: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
-# =============================================================================
-
-version: "3.9"
-
-services:
-  # ---------------------------------------------------------------------------
-  # Backend API Service - Production Configuration
-  # ---------------------------------------------------------------------------
-  backend:
-    build:
-      target: production
-    restart: always
-    deploy:
-      resources:
-        limits:
-          cpus: '2'
-          memory: 2G
-        reservations:
-          cpus: '0.5'
-          memory: 512M
-      replicas: 2
-    environment:
-      DEBUG: "false"
-      ENVIRONMENT: production
-      LOG_LEVEL: info
-      WORKERS: 4
-    volumes:
-      # Remove source code mount in production
-      - backend-logs:/app/logs
-    logging:
-      driver: "json-file"
-      options:
-        max-size: "10m"
-        max-file: "5"
-
-  # ---------------------------------------------------------------------------
-  # PostgreSQL Database - Production Configuration
-  # ---------------------------------------------------------------------------
-  db:
-    restart: always
-    deploy:
-      resources:
-        limits:
-          cpus: '2'
-          memory: 4G
-        reservations:
-          cpus: '0.5'
-          memory: 1G
-    command: >
-      postgres
-      -c max_connections=200
-      -c shared_buffers=1GB
-      -c effective_cache_size=3GB
-      -c maintenance_work_mem=256MB
-      -c checkpoint_completion_target=0.9
-      -c wal_buffers=16MB
-      -c default_statistics_target=100
-      -c random_page_cost=1.1
-      -c effective_io_concurrency=200
-      -c work_mem=5242kB
-      -c min_wal_size=1GB
-      -c max_wal_size=4GB
-      -c max_worker_processes=4
-      -c max_parallel_workers_per_gather=2
-      -c max_parallel_workers=4
-      -c max_parallel_maintenance_workers=2
-      -c log_statement=mod
-      -c log_duration=on
-      -c log_min_duration_statement=1000
-    logging:
-      driver: "json-file"
-      options:
-        max-size: "10m"
-        max-file: "5"
-
-  # ---------------------------------------------------------------------------
-  # Redis Cache - Production Configuration
-  # ---------------------------------------------------------------------------
-  redis:
-    restart: always
-    deploy:
-      resources:
-        limits:
-          cpus: '1'
-          memory: 512M
-        reservations:
-          cpus: '0.25'
-          memory: 128M
-    command: >
-      redis-server
-      --appendonly yes
-      --maxmemory 384mb
-      --maxmemory-policy allkeys-lru
-      --save 900 1
-      --save 300 10
-      --save 60 10000
-    logging:
-      driver: "json-file"
-      options:
-        max-size: "10m"
-        max-file: "3"
-
-  # ---------------------------------------------------------------------------
-  # Celery Worker - Production Configuration
-  # ---------------------------------------------------------------------------
-  celery-worker:
-    build:
-      target: production
-    restart: always
-    command: celery -A app.core.celery worker --loglevel=warning --concurrency=4
-    deploy:
-      resources:
-        limits:
-          cpus: '1'
-          memory: 1G
-        reservations:
-          cpus: '0.25'
-          memory: 256M
-      replicas: 2
-    volumes:
-      - backend-logs:/app/logs
-    logging:
-      driver: "json-file"
-      options:
-        max-size: "10m"
-        max-file: "3"
-
-  # ---------------------------------------------------------------------------
-  # Celery Beat - Production Configuration
-  # ---------------------------------------------------------------------------
-  celery-beat:
-    build:
-      target: production
-    restart: always
-    command: celery -A app.core.celery beat --loglevel=warning
-    deploy:
-      resources:
-        limits:
-          cpus: '0.5'
-          memory: 256M
-        reservations:
-          cpus: '0.1'
-          memory: 64M
-    volumes:
-      - backend-logs:/app/logs
-    logging:
-      driver: "json-file"
-      options:
-        max-size: "5m"
-        max-file: "3"
-
-  # ---------------------------------------------------------------------------
-  # Nginx Reverse Proxy (Production Only)
-  # ---------------------------------------------------------------------------
-  nginx:
-    image: nginx:alpine
-    container_name: br-capital-nginx
-    restart: always
-    ports:
-      - "80:80"
-      - "443:443"
-    volumes:
-      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
-      - ./nginx/ssl:/etc/nginx/ssl:ro
-      - nginx-logs:/var/log/nginx
-    depends_on:
-      - backend
-    networks:
-      - br-capital-network
-    deploy:
-      resources:
-        limits:
-          cpus: '0.5'
-          memory: 128M
-    logging:
-      driver: "json-file"
-      options:
-        max-size: "10m"
-        max-file: "5"
-    healthcheck:
-      test: ["CMD", "nginx", "-t"]
-      interval: 30s
-      timeout: 10s
-      retries: 3
-
-# =============================================================================
-# Additional Production Volumes
-# =============================================================================
-volumes:
-  nginx-logs:
-    name: br-capital-nginx-logs
diff --git a/backend/docker-compose.yml b/backend/docker-compose.yml
deleted file mode 100644
index ceb23b9..0000000
--- a/backend/docker-compose.yml
+++ /dev/null
@@ -1,210 +0,0 @@
-# =============================================================================
-# B&R Capital Dashboard Backend - Docker Compose Configuration
-# =============================================================================
-# Local development and testing environment
-# Usage:
-#   Development: docker-compose up -d
-#   Production:  docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
-# =============================================================================
-
-version: "3.9"
-
-services:
-  # ---------------------------------------------------------------------------
-  # Backend API Service
-  # ---------------------------------------------------------------------------
-  backend:
-    build:
-      context: .
-      dockerfile: Dockerfile
-      target: development
-    container_name: br-capital-backend
-    restart: unless-stopped
-    ports:
-      - "${API_PORT:-8000}:8000"
-    environment:
-      # Database
-      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@db:5432/${POSTGRES_DB:-br_capital}
-      # Redis
-      REDIS_URL: redis://redis:6379/0
-      # Application
-      SECRET_KEY: ${SECRET_KEY:-dev-secret-key-change-in-production}
-      DEBUG: ${DEBUG:-true}
-      ENVIRONMENT: ${ENVIRONMENT:-development}
-      LOG_LEVEL: ${LOG_LEVEL:-debug}
-      # CORS
-      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost:3000,http://localhost:8080}
-    volumes:
-      # Mount source code for hot reload in development
-      - .:/app
-      # Persist logs
-      - backend-logs:/app/logs
-    depends_on:
-      db:
-        condition: service_healthy
-      redis:
-        condition: service_healthy
-    networks:
-      - br-capital-network
-    healthcheck:
-      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
-      interval: 30s
-      timeout: 10s
-      retries: 3
-      start_period: 10s
-
-  # ---------------------------------------------------------------------------
-  # PostgreSQL Database
-  # ---------------------------------------------------------------------------
-  db:
-    image: postgres:15-alpine
-    container_name: br-capital-db
-    restart: unless-stopped
-    ports:
-      - "${DB_PORT:-5432}:5432"
-    environment:
-      POSTGRES_USER: ${POSTGRES_USER:-postgres}
-      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
-      POSTGRES_DB: ${POSTGRES_DB:-br_capital}
-      PGDATA: /var/lib/postgresql/data/pgdata
-    volumes:
-      - postgres-data:/var/lib/postgresql/data
-      # Optional: initialization scripts
-      - ./database/init:/docker-entrypoint-initdb.d:ro
-    networks:
-      - br-capital-network
-    healthcheck:
-      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-br_capital}"]
-      interval: 10s
-      timeout: 5s
-      retries: 5
-      start_period: 10s
-
-  # ---------------------------------------------------------------------------
-  # Redis Cache
-  # ---------------------------------------------------------------------------
-  redis:
-    image: redis:7-alpine
-    container_name: br-capital-redis
-    restart: unless-stopped
-    ports:
-      - "${REDIS_PORT:-6379}:6379"
-    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
-    volumes:
-      - redis-data:/data
-    networks:
-      - br-capital-network
-    healthcheck:
-      test: ["CMD", "redis-cli", "ping"]
-      interval: 10s
-      timeout: 5s
-      retries: 5
-      start_period: 5s
-
-  # ---------------------------------------------------------------------------
-  # Celery Worker (Background Tasks)
-  # ---------------------------------------------------------------------------
-  celery-worker:
-    build:
-      context: .
-      dockerfile: Dockerfile
-      target: development
-    container_name: br-capital-celery-worker
-    restart: unless-stopped
-    command: celery -A app.core.celery worker --loglevel=info --concurrency=2
-    environment:
-      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@db:5432/${POSTGRES_DB:-br_capital}
-      REDIS_URL: redis://redis:6379/0
-      CELERY_BROKER_URL: redis://redis:6379/1
-      CELERY_RESULT_BACKEND: redis://redis:6379/1
-    volumes:
-      - .:/app
-    depends_on:
-      - backend
-      - redis
-    networks:
-      - br-capital-network
-
-  # ---------------------------------------------------------------------------
-  # Celery Beat (Scheduled Tasks)
-  # ---------------------------------------------------------------------------
-  celery-beat:
-    build:
-      context: .
-      dockerfile: Dockerfile
-      target: development
-    container_name: br-capital-celery-beat
-    restart: unless-stopped
-    command: celery -A app.core.celery beat --loglevel=info
-    environment:
-      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@db:5432/${POSTGRES_DB:-br_capital}
-      REDIS_URL: redis://redis:6379/0
-      CELERY_BROKER_URL: redis://redis:6379/1
-      CELERY_RESULT_BACKEND: redis://redis:6379/1
-    volumes:
-      - .:/app
-    depends_on:
-      - celery-worker
-    networks:
-      - br-capital-network
-
-  # ---------------------------------------------------------------------------
-  # PgAdmin (Database Management) - Development Only
-  # ---------------------------------------------------------------------------
-  pgadmin:
-    image: dpage/pgadmin4:latest
-    container_name: br-capital-pgadmin
-    restart: unless-stopped
-    ports:
-      - "${PGADMIN_PORT:-5050}:80"
-    environment:
-      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@brcapital.com}
-      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
-      PGADMIN_CONFIG_SERVER_MODE: "False"
-    volumes:
-      - pgadmin-data:/var/lib/pgadmin
-    depends_on:
-      - db
-    networks:
-      - br-capital-network
-    profiles:
-      - dev-tools
-
-  # ---------------------------------------------------------------------------
-  # Redis Commander (Redis Management) - Development Only
-  # ---------------------------------------------------------------------------
-  redis-commander:
-    image: rediscommander/redis-commander:latest
-    container_name: br-capital-redis-commander
-    restart: unless-stopped
-    ports:
-      - "${REDIS_COMMANDER_PORT:-8081}:8081"
-    environment:
-      REDIS_HOSTS: local:redis:6379
-    depends_on:
-      - redis
-    networks:
-      - br-capital-network
-    profiles:
-      - dev-tools
-
-# =============================================================================
-# Networks
-# =============================================================================
-networks:
-  br-capital-network:
-    driver: bridge
-    name: br-capital-network
-
-# =============================================================================
-# Volumes
-# =============================================================================
-volumes:
-  postgres-data:
-    name: br-capital-postgres-data
-  redis-data:
-    name: br-capital-redis-data
-  backend-logs:
-    name: br-capital-backend-logs
-  pgadmin-data:
-    name: br-capital-pgadmin-data
diff --git a/backend/nginx/README.md b/backend/nginx/README.md
deleted file mode 100644
index 1251a0a..0000000
--- a/backend/nginx/README.md
+++ /dev/null
@@ -1,70 +0,0 @@
-# Nginx Configuration for B&R Capital Backend
-
-## Overview
-
-This directory contains the Nginx reverse proxy configuration for production deployment.
-
-## Files
-
-- `nginx.conf` - Main Nginx configuration
-- `ssl/` - SSL certificate directory (not tracked in git)
-
-## Features
-
-- **HTTPS Termination**: SSL/TLS encryption with modern cipher suites
-- **Load Balancing**: Least-connection algorithm for backend servers
-- **Rate Limiting**: 10 requests/second with burst handling
-- **WebSocket Support**: Real-time updates via `/ws` endpoint
-- **Gzip Compression**: Automatic compression for text-based responses
-- **Security Headers**: XSS protection, content-type sniffing prevention
-- **Health Checks**: Proxied health endpoint at `/health`
-
-## SSL Setup
-
-### Development (Self-Signed)
-
-```bash
-cd nginx/ssl
-openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
-  -keyout key.pem -out cert.pem \
-  -subj "/C=US/ST=Arizona/L=Phoenix/O=B&R Capital/CN=localhost"
-```
-
-### Production (Let's Encrypt)
-
-```bash
-# Install certbot
-sudo apt install certbot
-
-# Generate certificates
-sudo certbot certonly --standalone -d api.yourdomain.com
-
-# Copy to nginx/ssl/
-sudo cp /etc/letsencrypt/live/api.yourdomain.com/fullchain.pem nginx/ssl/cert.pem
-sudo cp /etc/letsencrypt/live/api.yourdomain.com/privkey.pem nginx/ssl/key.pem
-```
-
-## Usage
-
-Production deployment with nginx:
-
-```bash
-docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
-```
-
-## Endpoints
-
-| Path | Description |
-|------|-------------|
-| `/api/*` | API endpoints (rate limited) |
-| `/ws` | WebSocket connections |
-| `/docs` | Swagger UI documentation |
-| `/redoc` | ReDoc documentation |
-| `/health` | Health check endpoint |
-
-## Configuration Notes
-
-- Rate limiting: 10 req/s per IP (burst: 20)
-- Max request body: 50MB
-- WebSocket timeout: 7 days
-- SSL protocols: TLSv1.2, TLSv1.3
diff --git a/backend/nginx/nginx.conf b/backend/nginx/nginx.conf
deleted file mode 100644
index 7365e5d..0000000
--- a/backend/nginx/nginx.conf
+++ /dev/null
@@ -1,198 +0,0 @@
-# =============================================================================
-# B&R Capital Dashboard - Nginx Configuration
-# =============================================================================
-# Production reverse proxy configuration for FastAPI backend
-# =============================================================================
-
-worker_processes auto;
-error_log /var/log/nginx/error.log warn;
-pid /var/run/nginx.pid;
-
-events {
-    worker_connections 1024;
-    use epoll;
-    multi_accept on;
-}
-
-http {
-    include /etc/nginx/mime.types;
-    default_type application/octet-stream;
-
-    # Logging format
-    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
-                    '$status $body_bytes_sent "$http_referer" '
-                    '"$http_user_agent" "$http_x_forwarded_for" '
-                    'rt=$request_time uct="$upstream_connect_time" '
-                    'uht="$upstream_header_time" urt="$upstream_response_time"';
-
-    access_log /var/log/nginx/access.log main;
-
-    # Performance optimizations
-    sendfile on;
-    tcp_nopush on;
-    tcp_nodelay on;
-    keepalive_timeout 65;
-    types_hash_max_size 2048;
-
-    # Gzip compression
-    gzip on;
-    gzip_disable "msie6";
-    gzip_vary on;
-    gzip_proxied any;
-    gzip_comp_level 6;
-    gzip_buffers 16 8k;
-    gzip_http_version 1.1;
-    gzip_types text/plain text/css application/json application/javascript
-               text/xml application/xml application/xml+rss text/javascript;
-
-    # Security headers
-    add_header X-Frame-Options "SAMEORIGIN" always;
-    add_header X-Content-Type-Options "nosniff" always;
-    add_header X-XSS-Protection "1; mode=block" always;
-    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
-
-    # Rate limiting zone
-    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
-    limit_conn_zone $binary_remote_addr zone=addr:10m;
-
-    # Upstream backend servers
-    upstream backend {
-        least_conn;
-        server backend:8000 weight=1 max_fails=3 fail_timeout=30s;
-        keepalive 32;
-    }
-
-    # HTTP redirect to HTTPS
-    server {
-        listen 80;
-        server_name _;
-
-        # Health check endpoint (allow HTTP)
-        location /health {
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Connection "";
-        }
-
-        # Redirect all other traffic to HTTPS
-        location / {
-            return 301 https://$host$request_uri;
-        }
-    }
-
-    # HTTPS server
-    server {
-        listen 443 ssl http2;
-        server_name _;
-
-        # SSL Configuration
-        ssl_certificate /etc/nginx/ssl/cert.pem;
-        ssl_certificate_key /etc/nginx/ssl/key.pem;
-        ssl_session_timeout 1d;
-        ssl_session_cache shared:SSL:50m;
-        ssl_session_tickets off;
-
-        # Modern SSL configuration
-        ssl_protocols TLSv1.2 TLSv1.3;
-        ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
-        ssl_prefer_server_ciphers off;
-
-        # HSTS (optional - enable in production)
-        # add_header Strict-Transport-Security "max-age=63072000" always;
-
-        # Request size limits
-        client_max_body_size 50M;
-        client_body_buffer_size 128k;
-
-        # API endpoints
-        location /api {
-            limit_req zone=api burst=20 nodelay;
-            limit_conn addr 10;
-
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Upgrade $http_upgrade;
-            proxy_set_header Connection "upgrade";
-            proxy_set_header Host $host;
-            proxy_set_header X-Real-IP $remote_addr;
-            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
-            proxy_set_header X-Forwarded-Proto $scheme;
-            proxy_set_header X-Request-ID $request_id;
-
-            # Timeouts
-            proxy_connect_timeout 60s;
-            proxy_send_timeout 60s;
-            proxy_read_timeout 60s;
-
-            # Buffering
-            proxy_buffering on;
-            proxy_buffer_size 4k;
-            proxy_buffers 8 32k;
-            proxy_busy_buffers_size 64k;
-        }
-
-        # WebSocket support for real-time updates
-        location /ws {
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Upgrade $http_upgrade;
-            proxy_set_header Connection "upgrade";
-            proxy_set_header Host $host;
-            proxy_set_header X-Real-IP $remote_addr;
-            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
-
-            # WebSocket timeouts
-            proxy_connect_timeout 7d;
-            proxy_send_timeout 7d;
-            proxy_read_timeout 7d;
-        }
-
-        # Health check
-        location /health {
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Connection "";
-        }
-
-        # API documentation
-        location /docs {
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Host $host;
-            proxy_set_header X-Real-IP $remote_addr;
-            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
-            proxy_set_header X-Forwarded-Proto $scheme;
-        }
-
-        location /redoc {
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Host $host;
-            proxy_set_header X-Real-IP $remote_addr;
-            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
-            proxy_set_header X-Forwarded-Proto $scheme;
-        }
-
-        location /openapi.json {
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Host $host;
-        }
-
-        # Default location - proxy to backend
-        location / {
-            proxy_pass http://backend;
-            proxy_http_version 1.1;
-            proxy_set_header Host $host;
-            proxy_set_header X-Real-IP $remote_addr;
-            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
-            proxy_set_header X-Forwarded-Proto $scheme;
-        }
-
-        # Static files caching (if any)
-        location ~* \.(jpg|jpeg|png|gif|ico|css|js|pdf|txt)$ {
-            expires 7d;
-            add_header Cache-Control "public, immutable";
-        }
-    }
-}
diff --git a/backend/nginx/ssl/.gitkeep b/backend/nginx/ssl/.gitkeep
deleted file mode 100644
index a8654e9..0000000
--- a/backend/nginx/ssl/.gitkeep
+++ /dev/null
@@ -1,16 +0,0 @@
-# SSL Certificate Placeholder
-# =============================================================================
-# Place your SSL certificates here for production deployment:
-#
-#   cert.pem - SSL certificate
-#   key.pem  - SSL private key
-#
-# For development/testing, generate self-signed certificates:
-#   openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
-#     -keyout key.pem -out cert.pem \
-#     -subj "/C=US/ST=Arizona/L=Phoenix/O=B&R Capital/CN=localhost"
-#
-# For production, use certificates from:
-#   - Let's Encrypt (free, automated)
-#   - Commercial CA (DigiCert, Comodo, etc.)
-# =============================================================================
diff --git a/docker-compose.prod.yml b/docker-compose.prod.yml
deleted file mode 100644
index 90b4fa1..0000000
--- a/docker-compose.prod.yml
+++ /dev/null
@@ -1,158 +0,0 @@
-# =============================================================================
-# B&R Capital Dashboard - Production Docker Compose
-# =============================================================================
-# Production environment with security hardening and external secrets
-# Usage: docker-compose -f docker-compose.prod.yml up -d
-# =============================================================================
-
-version: '3.8'
-
-services:
-  # ---------------------------------------------------------------------------
-  # PostgreSQL Database
-  # ---------------------------------------------------------------------------
-  postgres:
-    image: postgres:15-alpine
-    container_name: dashboard-postgres-prod
-    environment:
-      POSTGRES_USER: ${POSTGRES_USER}
-      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
-      POSTGRES_DB: ${POSTGRES_DB:-dashboard_interface_data}
-    volumes:
-      - postgres_data_prod:/var/lib/postgresql/data
-    healthcheck:
-      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
-      interval: 10s
-      timeout: 5s
-      retries: 5
-      start_period: 30s
-    networks:
-      - dashboard-network-prod
-    restart: always
-    deploy:
-      resources:
-        limits:
-          cpus: '1'
-          memory: 1G
-        reservations:
-          cpus: '0.5'
-          memory: 512M
-
-  # ---------------------------------------------------------------------------
-  # Redis Cache
-  # ---------------------------------------------------------------------------
-  redis:
-    image: redis:7-alpine
-    container_name: dashboard-redis-prod
-    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
-    volumes:
-      - redis_data_prod:/data
-    healthcheck:
-      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
-      interval: 10s
-      timeout: 5s
-      retries: 5
-      start_period: 10s
-    networks:
-      - dashboard-network-prod
-    restart: always
-    deploy:
-      resources:
-        limits:
-          cpus: '0.5'
-          memory: 512M
-        reservations:
-          cpus: '0.25'
-          memory: 256M
-
-  # ---------------------------------------------------------------------------
-  # Backend API (FastAPI)
-  # ---------------------------------------------------------------------------
-  backend:
-    build:
-      context: ./backend
-      target: production
-    container_name: dashboard-backend-prod
-    environment:
-      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-dashboard_interface_data}
-      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
-      - SECRET_KEY=${SECRET_KEY}
-      - ENVIRONMENT=production
-      - DEBUG=false
-      - LOG_LEVEL=info
-      - HOST=0.0.0.0
-      - PORT=8000
-      - WORKERS=${BACKEND_WORKERS:-4}
-    expose:
-      - "8000"
-    depends_on:
-      postgres:
-        condition: service_healthy
-      redis:
-        condition: service_healthy
-    networks:
-      - dashboard-network-prod
-    restart: always
-    deploy:
-      resources:
-        limits:
-          cpus: '2'
-          memory: 2G
-        reservations:
-          cpus: '1'
-          memory: 1G
-    healthcheck:
-      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
-      interval: 30s
-      timeout: 10s
-      retries: 3
-      start_period: 30s
-
-  # ---------------------------------------------------------------------------
-  # Frontend (React + Vite via Nginx)
-  # ---------------------------------------------------------------------------
-  frontend:
-    build:
-      context: .
-      dockerfile: Dockerfile.frontend
-      target: production
-    container_name: dashboard-frontend-prod
-    ports:
-      - "${FRONTEND_PORT:-80}:80"
-    depends_on:
-      backend:
-        condition: service_healthy
-    networks:
-      - dashboard-network-prod
-    restart: always
-    deploy:
-      resources:
-        limits:
-          cpus: '0.5'
-          memory: 256M
-        reservations:
-          cpus: '0.25'
-          memory: 128M
-    healthcheck:
-      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
-      interval: 30s
-      timeout: 5s
-      retries: 3
-      start_period: 10s
-
-# =============================================================================
-# Networks
-# =============================================================================
-networks:
-  dashboard-network-prod:
-    driver: bridge
-    name: dashboard-network-prod
-
-# =============================================================================
-# Volumes
-# =============================================================================
-volumes:
-  postgres_data_prod:
-    name: dashboard-postgres-data-prod
-  redis_data_prod:
-    name: dashboard-redis-data-prod
diff --git a/docker-compose.yml b/docker-compose.yml
deleted file mode 100644
index ef1d0d2..0000000
--- a/docker-compose.yml
+++ /dev/null
@@ -1,120 +0,0 @@
-# =============================================================================
-# B&R Capital Dashboard - Development Docker Compose
-# =============================================================================
-# Local development environment with hot-reload and debugging support
-# Usage: docker-compose up -d
-# =============================================================================
-
-version: '3.8'
-
-services:
-  # ---------------------------------------------------------------------------
-  # PostgreSQL Database
-  # ---------------------------------------------------------------------------
-  postgres:
-    image: postgres:15-alpine
-    container_name: dashboard-postgres
-    environment:
-      POSTGRES_USER: postgres
-      POSTGRES_PASSWORD: postgres123
-      POSTGRES_DB: dashboard_interface_data
-    volumes:
-      - postgres_data:/var/lib/postgresql/data
-    ports:
-      - "5432:5432"
-    healthcheck:
-      test: ["CMD-SHELL", "pg_isready -U postgres"]
-      interval: 5s
-      timeout: 5s
-      retries: 5
-    networks:
-      - dashboard-network
-    restart: unless-stopped
-
-  # ---------------------------------------------------------------------------
-  # Redis Cache
-  # ---------------------------------------------------------------------------
-  redis:
-    image: redis:7-alpine
-    container_name: dashboard-redis
-    ports:
-      - "6379:6379"
-    volumes:
-      - redis_data:/data
-    healthcheck:
-      test: ["CMD", "redis-cli", "ping"]
-      interval: 5s
-      timeout: 5s
-      retries: 5
-    networks:
-      - dashboard-network
-    restart: unless-stopped
-
-  # ---------------------------------------------------------------------------
-  # Backend API (FastAPI)
-  # ---------------------------------------------------------------------------
-  backend:
-    build:
-      context: ./backend
-      target: development
-    container_name: dashboard-backend
-    environment:
-      - DATABASE_URL=postgresql://postgres:postgres123@postgres:5432/dashboard_interface_data
-      - REDIS_URL=redis://redis:6379/0
-      - SECRET_KEY=dev-secret-key-change-in-production
-      - ENVIRONMENT=development
-      - DEBUG=true
-      - LOG_LEVEL=debug
-      - HOST=0.0.0.0
-      - PORT=8000
-    ports:
-      - "8000:8000"
-    volumes:
-      - ./backend:/app
-      - backend_logs:/app/logs
-    depends_on:
-      postgres:
-        condition: service_healthy
-      redis:
-        condition: service_healthy
-    networks:
-      - dashboard-network
-    restart: unless-stopped
-
-  # ---------------------------------------------------------------------------
-  # Frontend (React + Vite via Nginx)
-  # ---------------------------------------------------------------------------
-  frontend:
-    build:
-      context: .
-      dockerfile: Dockerfile.frontend
-      target: production
-    container_name: dashboard-frontend
-    ports:
-      - "5173:80"
-    depends_on:
-      - backend
-    environment:
-      - VITE_API_URL=http://localhost:8000
-    networks:
-      - dashboard-network
-    restart: unless-stopped
-
-# =============================================================================
-# Networks
-# =============================================================================
-networks:
-  dashboard-network:
-    driver: bridge
-    name: dashboard-network
-
-# =============================================================================
-# Volumes
-# =============================================================================
-volumes:
-  postgres_data:
-    name: dashboard-postgres-data
-  redis_data:
-    name: dashboard-redis-data
-  backend_logs:
-    name: dashboard-backend-logs
diff --git a/nginx.conf b/nginx.conf
deleted file mode 100644
index 657b05e..0000000
--- a/nginx.conf
+++ /dev/null
@@ -1,47 +0,0 @@
-server {
-    listen 80;
-    server_name localhost;
-    root /usr/share/nginx/html;
-    index index.html;
-
-    # Gzip compression
-    gzip on;
-    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;
-    gzip_min_length 1000;
-
-    # Security headers
-    add_header X-Frame-Options "SAMEORIGIN" always;
-    add_header X-Content-Type-Options "nosniff" always;
-    add_header X-XSS-Protection "1; mode=block" always;
-
-    # Handle SPA routing - serve index.html for all routes
-    location / {
-        try_files $uri $uri/ /index.html;
-    }
-
-    # Cache static assets
-    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2)$ {
-        expires 1y;
-        add_header Cache-Control "public, immutable";
-    }
-
-    # API proxy (if needed)
-    location /api/ {
-        proxy_pass http://backend:8000/api/;
-        proxy_http_version 1.1;
-        proxy_set_header Upgrade $http_upgrade;
-        proxy_set_header Connection 'upgrade';
-        proxy_set_header Host $host;
-        proxy_set_header X-Real-IP $remote_addr;
-        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
-        proxy_set_header X-Forwarded-Proto $scheme;
-        proxy_cache_bypass $http_upgrade;
-    }
-
-    # Health check endpoint
-    location /health {
-        access_log off;
-        return 200 'healthy';
-        add_header Content-Type text/plain;
-    }
-}
diff --git a/src/components/GlobalSearch.tsx b/src/components/GlobalSearch.tsx
index 337f023..b9c0ad2 100644
--- a/src/components/GlobalSearch.tsx
+++ b/src/components/GlobalSearch.tsx
@@ -267,9 +267,18 @@ export function GlobalSearch() {
     ];
   }, [debouncedQuery, properties]);
 
-  // Update results in store
+  // Update results in store only when content actually changes
+  const resultsRef = useRef<SearchResult[]>([]);
   useEffect(() => {
-    setResults(results);
+    // Only update if results actually changed (compare by length and IDs)
+    const hasChanged =
+      results.length !== resultsRef.current.length ||
+      results.some((r, i) => r.id !== resultsRef.current[i]?.id);
+
+    if (hasChanged) {
+      resultsRef.current = results;
+      setResults(results);
+    }
   }, [results, setResults]);
 
   // Keyboard shortcuts
